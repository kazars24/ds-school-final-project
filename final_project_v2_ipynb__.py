# -*- coding: utf-8 -*-
"""Final_Project_v2.ipynb""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kqJoZuxLAa_oSNRDLbYPJwkLCK9ifIZ5

# <center>**Финальный проект** </center>

## Постановка задачи

Предсказать, является ли рак доброкачественным или злокачественным.
Целевая переменная *diagnosis* имеет два значения: B и M (доброкачественная и злокачественная соответственно).

## Описание данных

- `id` -- идентификационный номер каждого наблюдения.
- `diagnosis` -- целевая переменная.
- `radius` -- среднее расстояние от центра до точек по периметру.
- `texture` -- стандартное отклонение значений серой шкалы.
- `perimeter` -- периметр клетки.
- `area` -- площадь клетки.
- `smoothness` -- локальное изменение длин радиусов.
- `compactness` -- периметр^2 / площадь - 1.0.
- `concavity` -- выраженность вогнутых участков контура.
- `concave points` -- количество вогнутых участков контура.
- `symmetry` -- симметрия.
- `fractal_dimension` -- "приближение береговой линии" - 1.

Среднее значение, стандартная ошибка и "наихудшее" или наибольшее (среднее из трех
наибольших значений) этих признаков были вычислены для каждого изображения, в
результате чего было получено 30 признаков. Например, поле 3-это средний радиус, поле
13-Радиус SE, поле 23-Наихудший радиус.

##  Первичный анализ данных
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import division, print_function
# отключим всякие предупреждения Anaconda
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
# %pylab inline

df = pd.read_csv("data.csv")
print(df.shape)
df.head()

df.info()

df.isnull().sum()

df.describe()

"""Так как `id` не пригодится в построении модели, то этот признак можно удалить.

Признак `Unnamed: 32` не несет абсолютно никакой информации, а всего его значения это NaN, то этот признак тоже можно удалить.
"""

df = df.drop(['Unnamed: 32','id'], axis = 1)
df.shape

"""## Первичный визуальный анализ признаков"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt
#графики в svg выглядят более четкими
# %config InlineBackend.figure_format = 'svg' 

sns.set(context='notebook', style='darkgrid', palette='deep',
        font='sans-serif', font_scale=1, color_codes=False, rc=None)

cols = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',
        'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
        'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

sns.pairplot(df[cols], hue="diagnosis", palette='inferno')
plt.savefig('pairplot_mean1.png')

cols = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',
        'area_mean', 'smoothness_mean']

sns.pairplot(df[cols], hue="diagnosis", palette='inferno')
plt.savefig('pairplot_mean2.png')

cols = ['diagnosis', 'radius_se', 'texture_se', 'perimeter_se',
        'area_se', 'smoothness_se']
sns.pairplot(df[cols], hue="diagnosis", palette='inferno')
plt.savefig('pairplot_se.png')

j = 0
for i in df.columns:
  j = j+1
  plt.figure(figsize = (5,5))
  plt.title(i)
  plt.hist(df[i],30)

sns.countplot(x='diagnosis', data=df, palette='inferno')
plt.title('Counting in each class')
plt.savefig('countplot.png')

f, ax=plt.subplots(figsize = (15,15))
sns.heatmap(df.corr(), annot=True, linewidths=0.5, fmt=".1f", ax=ax)
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Map')
plt.show()
plt.savefig('heatmap.png')

"""По графикам можно заметить, что такие фичи, как радиус, периметр и площадь (особенно средние значения) связаны между собой и находятся в прямой зависимости друг от друга, соответственно они сильно скоррелированные (1.0).

Кроме того, среднее значение, стандартная ошибка и "наихудшее" у некоторых признаков тоже сильно скорррелированные.

Наблюдений класса B больше чем M, но значения класса B у многих фичей выше, чем у класса M.
"""

from sklearn.model_selection import train_test_split

df, holdout = train_test_split(df, test_size = 0.2, random_state = 17)

# проверка на сбалансированность обучающей и тестовой выборок
plt.figure(figsize=(15, 6))

plt.subplot(121)
sns.countplot(x=df['diagnosis'], data=df, palette='inferno')
plt.title('df')

plt.subplot(122)
sns.countplot(x=holdout['diagnosis'], data=holdout, palette='inferno')
plt.title('holdout')

plt.show()
plt.savefig('balance.png')

y = df['diagnosis']
X = df.drop('diagnosis', axis = 1)
y_holdout = holdout['diagnosis']
X_holdout = holdout.drop('diagnosis', axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,
                                                    stratify = y,
                                                    random_state = 17)

def correlation(data, limit):
    col_corr = set() # набор имен столбцов
    corr_matrix = data.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if (corr_matrix.iloc[i, j]) > limit: 
                col_name = corr_matrix.columns[i] # получение имени столбца
                col_corr.add(col_name)
    return col_corr

corr_features = correlation(X_train, 0.9)
print(len(corr_features))
print(corr_features)
# удаляем сильно скоррелированные фичи
X_train = X_train.drop(corr_features, axis=1) 
X_train.head()

X_train.info()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train_columns = X_train.columns
X_train=scaler.fit_transform(X_train)
X_train

X_train = pd.DataFrame(X_train, columns=X_train_columns)
X_train.head()

# проверка на сбалансированность обучающей и тестовой выборок
plt.figure(figsize=(15, 6))

plt.subplot(121)
sns.countplot(x=y_train, data=X_train, palette='inferno')
plt.title('X_train')

plt.subplot(122)
sns.countplot(x=y_test, data=X_test, palette='inferno')
plt.title('X_test')

plt.show()
plt.savefig('balance.png')

from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer

def drop_func(data):
  return data.drop(corr_features, axis=1)

vec = make_union(*[
    make_pipeline(FunctionTransformer(drop_func, validate=False),
                  scaler)])

X_test = vec.transform(X_test)
X_test = pd.DataFrame(X_test, columns=X_train_columns)

X_test.head()

X_holdout = vec.transform(X_holdout)
X_holdout = pd.DataFrame(X_holdout, columns=X_train_columns)

X_holdout.head()

"""##  Часть 2. Обучение модели классификации

#### **KNN**
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, RandomizedSearchCV

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)

param_grid = [
              {'n_neighbors': range(1, 15),
               'weights': ['uniform', 'distance'],
               'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
               'leaf_size': range(1, 25),
               'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski',
                          'seuclidean', 'mahalanobis']}
]

knn_cv = GridSearchCV(KNeighborsClassifier(), param_grid, cv=skf,
                           scoring='roc_auc', verbose=True, n_jobs=-1)
knn_cv.fit(X_train, y_train)

knn_cv.best_params_

knn_cv.best_score_

y_pred = knn_cv.best_estimator_.predict_proba(X_test)
roc_auc_score(y_test, y_pred[:, 1])

holdout_pred = knn_cv.best_estimator_.predict_proba(X_holdout)
roc_auc_score(y_holdout, holdout_pred[:, 1])

knn_best_params = knn_cv.best_params_.copy()
del knn_best_params['n_neighbors']

train_roc_auc, test_roc_auc, holdout_roc_auc = [], [], []
for n in  range(1, 15):
  knn = KNeighborsClassifier(**knn_best_params, n_neighbors=n)
  train_roc_auc.append(mean(cross_val_score(knn, X_train, y_train, cv=skf,
                                            scoring='roc_auc')))
  
  knn.fit(X_train, y_train)
  y_pred = knn.predict_proba(X_test)
  test_score = roc_auc_score(y_test, y_pred[:, 1])
  test_roc_auc.append(test_score)

  holdout_pred = knn.predict_proba(X_holdout)
  holdout_score = roc_auc_score(y_holdout, holdout_pred[:, 1])
  holdout_roc_auc.append(holdout_score)

print(train_roc_auc)
print(test_roc_auc)
print(holdout_roc_auc)

plt.figure(figsize = (10,5))
plt.plot(range(1, 15), train_roc_auc, 'o-', label='train')
plt.plot(range(1, 15), test_roc_auc, 'o-', label='test')
plt.plot(range(1, 15), holdout_roc_auc, 'o-', label='holdout')
plt.xlabel('n_neighbors')
plt.ylabel('roc auc score')
plt.legend(loc="lower right")
plt.show()

"""### **LogisticRegression**"""

from sklearn.linear_model import LogisticRegression

params = [
          {'penalty': ['l1', 'l2', 'elasticnet', 'none'],
           'dual': [True, False],
           'tol': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
           'C': np.array([0.0001, 0.001, 0.01, 0.1, 1, 10]),
           'fit_intercept': [True, False],
           'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
           'random_state': [17],
           'multi_class': ['auto', 'ovr', 'multinomial']}
]

logreg_cv = GridSearchCV(LogisticRegression(), params, cv=skf, 
                           scoring='roc_auc', verbose=True, n_jobs=-1)
logreg_cv.fit(X_train, y_train)

logreg_cv.best_params_

logreg_cv.best_score_

y_pred = logreg_cv.best_estimator_.predict_proba(X_test)
roc_auc_score(y_test, y_pred[:, 1])

holdout_pred = logreg_cv.best_estimator_.predict_proba(X_holdout)
roc_auc_score(y_holdout, holdout_pred[:, 1])

logreg_best_params = logreg_cv.best_params_.copy()
del logreg_best_params['C']

train_roc_auc, test_roc_auc, holdout_roc_auc = [], [], []
for c in np.array([0.0001, 0.001, 0.01, 0.1, 1, 10]):
  lg = LogisticRegression(**logreg_best_params, C=c)
  train_roc_auc.append(mean(cross_val_score(lg, X_train, y_train, cv=skf,
                                            scoring='roc_auc')))
  
  lg.fit(X_train, y_train)
  y_pred = lg.predict_proba(X_test)
  test_score = roc_auc_score(y_test, y_pred[:, 1])
  test_roc_auc.append(test_score)

  holdout_pred = lg.predict_proba(X_holdout)
  holdout_score = roc_auc_score(y_holdout, holdout_pred[:, 1])
  holdout_roc_auc.append(holdout_score)

print(train_roc_auc)
print(test_roc_auc)
print(holdout_roc_auc)

plt.figure(figsize = (10,5))
plt.plot(np.array([0.0001, 0.001, 0.01, 0.1, 1, 10]), train_roc_auc, 'o-', label='train')
plt.plot(np.array([0.0001, 0.001, 0.01, 0.1, 1, 10]), test_roc_auc, 'o-', label='test')
plt.plot(np.array([0.0001, 0.001, 0.01, 0.1, 1, 10]), holdout_roc_auc, 'o-', label='holdout')
plt.xlabel('C')
plt.ylabel('roc auc score')
plt.legend(loc="lower right")
plt.show()

"""### **RandomForest**"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state= 17, n_jobs=-1)

parameters = {'n_estimators':  range(50,1000,50),
              'criterion': ['gini', 'entropy'],
              'max_depth': range(1,7),
              'min_samples_split': range(1,15),
              'min_samples_leaf': range(1,15)
}
rf_cv = RandomizedSearchCV(rf, parameters, cv=skf, scoring='roc_auc',
                           verbose=True, n_jobs=-1, n_iter=250)
rf_cv.fit(X_train, y_train)

rf_cv.best_params_

rf_cv.best_score_

y_pred = rf_cv.best_estimator_.predict_proba(X_test)
roc_auc_score(y_test, y_pred[:, 1])

holdout_pred =rf_cv.predict_proba(X_holdout)
roc_auc_score(y_holdout, holdout_pred[:, 1])

rf_best_params = rf_cv.best_params_.copy()
del rf_best_params['max_depth']

train_roc_auc, test_roc_auc, holdout_roc_auc = [], [], []
for depth in range(1,7):
  rf = RandomForestClassifier(**rf_best_params, max_depth=depth,
                              random_state= 17, n_jobs=-1)
  train_roc_auc.append(mean(cross_val_score(rf, X_train, y_train, cv=skf,
                                            scoring='roc_auc')))
  
  rf.fit(X_train, y_train)
  y_pred = rf.predict_proba(X_test)
  test_score = roc_auc_score(y_test, y_pred[:, 1])
  test_roc_auc.append(test_score)

  holdout_pred = rf.predict_proba(X_holdout)
  holdout_score = roc_auc_score(y_holdout, holdout_pred[:, 1])
  holdout_roc_auc.append(holdout_score)

print(train_roc_auc)
print(test_roc_auc)
print(holdout_roc_auc)

plt.figure(figsize = (10,5))
plt.plot(range(1,7), train_roc_auc, 'o-', label='train')
plt.plot(range(1,7), test_roc_auc, 'o-', label='test')
plt.plot(range(1,7), holdout_roc_auc, 'o-', label='holdout')
plt.xlabel('max_depth')
plt.ylabel('roc auc score')
plt.legend(loc="lower right")
plt.show()

"""### **Catboost**"""

!pip install -q catboost

import catboost
from catboost import CatBoostClassifier

cbc = CatBoostClassifier(random_seed=17, eval_metric='AUC')

parameters = {'iterations': [100, 250, 500, 1000],
              'depth': [1, 2, 3, 4, 5, 6],
              'l2_leaf_reg': [0.2, 0.5, 1, 3],
              'learning_rate': [0.001, 0.1, 1, 10],
              'loss_function': ['Logloss', 'CrossEntropy']
}

cbc_cv = GridSearchCV(cbc, parameters, cv=skf, scoring='roc_auc', verbose=True,
                     n_jobs=-1)
cbc_cv.fit(X_train, y_train)

cbc_cv.best_score_

cbc_cv.best_params_

y_pred = cbc_cv.best_estimator_.predict_proba(X_test)
roc_auc_score(y_test, y_pred[:, 1])

holdout_pred =cbc_cv.predict_proba(X_holdout)
roc_auc_score(y_holdout, holdout_pred[:, 1])

cbc_best_params = cbc_cv.best_params_.copy()
del cbc_best_params['depth']

train_roc_auc, test_roc_auc, holdout_roc_auc = [], [], []
for d in [1, 2, 3, 4, 5, 6]:
  cbc = CatBoostClassifier(**cbc_best_params, random_seed=17, eval_metric='AUC', depth=d)
  train_roc_auc.append(mean(cross_val_score(cbc, X_train, y_train, cv=skf,
                                            scoring='roc_auc')))
  
  cbc.fit(X_train, y_train)
  y_pred = cbc.predict_proba(X_test)
  test_score = roc_auc_score(y_test, y_pred[:, 1])
  test_roc_auc.append(test_score)

  holdout_pred = cbc.predict_proba(X_holdout)
  holdout_score = roc_auc_score(y_holdout, holdout_pred[:, 1])
  holdout_roc_auc.append(holdout_score)

print(train_roc_auc)
print(test_roc_auc)
print(holdout_roc_auc)

plt.figure(figsize = (10,5))
plt.plot([1, 2, 3, 4, 5, 6], train_roc_auc, 'o-', label='train')
plt.plot([1, 2, 3, 4, 5, 6], test_roc_auc, 'o-', label='test')
plt.plot([1, 2, 3, 4, 5, 6], holdout_roc_auc, 'o-', label='holdout')
plt.xlabel('depth')
plt.ylabel('roc auc score')
plt.legend(loc="lower right")
plt.show()

"""###  Часть 3. Выбор наилучшей модели"""

from sklearn.metrics import accuracy_score
import time

start_time = time.time()
y_pred_knn = knn_cv.best_estimator_.predict_proba(X_test)
#roc_auc_score(y_test, y_pred_knn[:, 1])
holdout_pred_knn = knn_cv.best_estimator_.predict_proba(X_holdout)
#roc_auc_score(y_holdout, holdout_pred_knn[:, 1])
knn_time = time.time() - start_time

start_time = time.time()
y_pred_logreg = logreg_cv.best_estimator_.predict_proba(X_test)
#roc_auc_score(y_test, y_pred_logreg[:, 1])
holdout_pred_logreg = logreg_cv.best_estimator_.predict_proba(X_holdout)
#roc_auc_score(y_holdout, holdout_pred_logreg[:, 1])
logreg_time = time.time() - start_time

start_time = time.time()
y_pred_rf = rf_cv.best_estimator_.predict_proba(X_test)
#roc_auc_score(y_test, y_pred_rf[:, 1])
holdout_pred_rf = rf_cv.best_estimator_.predict_proba(X_holdout)
#roc_auc_score(y_holdout, holdout_pred_rf[:, 1])
rf_time = time.time() - start_time

start_time = time.time()
y_pred_cbc = cbc_cv.best_estimator_.predict_proba(X_test)
#roc_auc_score(y_test, y_pred_cbc[:, 1])
holdout_pred_cbc = cbc_cv.best_estimator_.predict_proba(X_holdout)
#roc_auc_score(y_holdout, holdout_pred_cbc[:, 1])
cbc_time = time.time() - start_time

"""Для начала сравним roc auc score на тестовой части"""

models = pd.DataFrame({
    'Model': ['KNN','Logistic Regression','Random Forest', 'CatBoostClassifier'],
    'ROC AUC': [roc_auc_score(y_test, y_pred_knn[:, 1]),
                      roc_auc_score(y_test, y_pred_logreg[:, 1]),
                      roc_auc_score(y_test, y_pred_rf[:, 1]),
                      roc_auc_score(y_test, y_pred_cbc[:, 1])]})

models.sort_values(by='ROC AUC', ascending=False)

"""Логистическая регрессия показывает наилучший результат

Теперь сравним тоже самое для holdout
"""

models = pd.DataFrame({
    'Model': ['KNN','Logistic Regression','Random Forest', 'CatBoostClassifier'],
    'ROC AUC': [roc_auc_score(y_holdout, holdout_pred_knn[:, 1]),
                      roc_auc_score(y_holdout, holdout_pred_logreg[:, 1]),
                      roc_auc_score(y_holdout, holdout_pred_rf[:, 1]),
                      roc_auc_score(y_holdout, holdout_pred_cbc[:, 1])]})

models.sort_values(by='ROC AUC', ascending=False)

"""Логистическая регрессия показывает наилучший результат

На данном этапе сравним точности моделей на X_test
"""

models = pd.DataFrame({
    'Model': ['KNN','Logistic Regression','Random Forest', 'CatBoostClassifier'],
    'Accuracy': [accuracy_score(y_test, knn_cv.predict(X_test)),
                 accuracy_score(y_test, logreg_cv.predict(X_test)),
                 accuracy_score(y_test, rf_cv.predict(X_test)),
                 accuracy_score(y_test, cbc_cv.predict(X_test))]})

models.sort_values(by='Accuracy', ascending=False)

"""Наиболее точной оказались KNN и логистическая регрессия

Подобное сравнение и для holdout
"""

models = pd.DataFrame({
    'Model': ['KNN','Logistic Regression','Random Forest', 'CatBoostClassifier'],
    'Accuracy': [accuracy_score(y_holdout, knn_cv.predict(X_holdout)),
                 accuracy_score(y_holdout, logreg_cv.predict(X_holdout)),
                 accuracy_score(y_holdout, rf_cv.predict(X_holdout)),
                 accuracy_score(y_holdout, cbc_cv.predict(X_holdout))]})

models.sort_values(by='Accuracy', ascending=False)

"""Тут лучшей модель - Логистическая регрессия

Далее проведем сравнение времени, которое уходит на предсказание test и holdout
"""

models = pd.DataFrame({
    'Model': ['K - Nearest Neighbors','Logistic Regression','Random Forest', 'Gradient Boosting'],
    'Time': [knn_time, logreg_time, rf_time, cbc_time]})

models.sort_values(by='Time', ascending=True)

"""По веремени предсказания самой быстрой оказалась Логистическая регрессия

**Вывод**

В совокупности всех приведенных сравнений, наилучшая модель в рамках данной задачи - Logistic Regression
"""